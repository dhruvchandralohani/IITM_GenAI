{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7huhCdq60NA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import os\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "z_dim = 100  # Size of the noise vector\n",
        "image_size = 28\n",
        "channels = 1\n",
        "epochs = 50\n",
        "lr = 0.0002\n",
        "beta1 = 0.5  # Adam optimizer beta1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create output folder\n",
        "os.makedirs(\"generated_images\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])  # Normalize between [-1, 1]\n",
        "])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('.', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "xp_G9DoA7yTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54dce294-5710-4138-9a98-bd83dfb7b19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 16.3MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 494kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 4.50MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 4.97MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: (N, z_dim, 1, 1)\n",
        "            nn.ConvTranspose2d(z_dim, 256, 7, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 1, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()  # Output range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)"
      ],
      "metadata": {
        "id": "56tgfJEr706L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: (N, 1, 28, 28)\n",
        "            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 7 * 7, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.net(img)"
      ],
      "metadata": {
        "id": "HkaE-2TM75r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator(z_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Binary Cross Entropy loss\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "metadata": {
        "id": "nrN7ULtC78On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(epoch):\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(64, z_dim, 1, 1).to(device)\n",
        "        fake_images = generator(z)\n",
        "        fake_images = fake_images * 0.5 + 0.5  # Denormalize to [0,1]\n",
        "        save_image(fake_images, f\"generated_images/sample_epoch_{epoch}.png\", nrow=8)\n",
        "    generator.train()"
      ],
      "metadata": {
        "id": "g2_yjsuO7_vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3  # Generator updates per iteration\n",
        "p = 1  # Discriminator updates per iteration"
      ],
      "metadata": {
        "id": "FnT4Il_S-2UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs + 1):\n",
        "    for i, (real_imgs, _) in enumerate(train_loader):\n",
        "        batch_size_curr = real_imgs.size(0)\n",
        "        real_imgs = real_imgs.to(device)\n",
        "\n",
        "        real = torch.ones(batch_size_curr, 1, device=device)\n",
        "        fake = torch.zeros(batch_size_curr, 1, device=device)\n",
        "\n",
        "        ### ---- Train Discriminator p times ---- ###\n",
        "        for _ in range(p):\n",
        "            z = torch.randn(batch_size_curr, z_dim, 1, 1, device=device)\n",
        "            fake_imgs = generator(z)\n",
        "\n",
        "            # Real\n",
        "            real_validity = discriminator(real_imgs)\n",
        "            d_real_loss = criterion(real_validity, real)\n",
        "\n",
        "            # Fake\n",
        "            fake_validity = discriminator(fake_imgs.detach())\n",
        "            d_fake_loss = criterion(fake_validity, fake)\n",
        "\n",
        "            d_loss = d_real_loss + d_fake_loss\n",
        "\n",
        "            optimizer_D.zero_grad()\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "        ### ---- Train Generator k times ---- ###\n",
        "        for _ in range(k):\n",
        "            z = torch.randn(batch_size_curr, z_dim, 1, 1, device=device)\n",
        "            fake_imgs = generator(z)\n",
        "\n",
        "            validity = discriminator(fake_imgs)\n",
        "            g_loss = criterion(validity, real)  # fool D â†’ label as real\n",
        "\n",
        "            optimizer_G.zero_grad()\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "        if i % 200 == 0:\n",
        "            print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(train_loader)}] \"\n",
        "                  f\"D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    # Save sample images\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(64, z_dim, 1, 1, device=device)\n",
        "        samples = generator(z)\n",
        "        samples = samples * 0.5 + 0.5  # Denormalize\n",
        "        save_image(samples, f\"generated_images/epoch_{epoch}.png\", nrow=8)\n",
        "    generator.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "oiXv5iy88E_0",
        "outputId": "5a6c0f9c-d763-489d-e397-351b008c8425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/50] [Batch 0/469] D Loss: 1.4766 | G Loss: 0.3533\n",
            "[Epoch 1/50] [Batch 200/469] D Loss: 1.2782 | G Loss: 1.0192\n",
            "[Epoch 1/50] [Batch 400/469] D Loss: 1.2132 | G Loss: 0.8656\n",
            "[Epoch 2/50] [Batch 0/469] D Loss: 1.1569 | G Loss: 0.7870\n",
            "[Epoch 2/50] [Batch 200/469] D Loss: 1.1923 | G Loss: 0.8132\n",
            "[Epoch 2/50] [Batch 400/469] D Loss: 1.2368 | G Loss: 0.8995\n",
            "[Epoch 3/50] [Batch 0/469] D Loss: 1.2196 | G Loss: 0.9452\n",
            "[Epoch 3/50] [Batch 200/469] D Loss: 1.1424 | G Loss: 0.8786\n",
            "[Epoch 3/50] [Batch 400/469] D Loss: 1.1655 | G Loss: 0.9409\n",
            "[Epoch 4/50] [Batch 0/469] D Loss: 1.1530 | G Loss: 0.9051\n",
            "[Epoch 4/50] [Batch 200/469] D Loss: 1.2202 | G Loss: 0.4291\n",
            "[Epoch 4/50] [Batch 400/469] D Loss: 1.1605 | G Loss: 0.8143\n",
            "[Epoch 5/50] [Batch 0/469] D Loss: 1.1694 | G Loss: 0.8676\n",
            "[Epoch 5/50] [Batch 200/469] D Loss: 1.2300 | G Loss: 0.7307\n",
            "[Epoch 5/50] [Batch 400/469] D Loss: 1.0831 | G Loss: 0.6974\n",
            "[Epoch 6/50] [Batch 0/469] D Loss: 1.1577 | G Loss: 1.0701\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a88733561516>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NgHex9GJhkkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "def generate_images_dcgan_individually(generator, num_samples=512, save_dir=\"generated_images\", prefix=\"img\"):\n",
        "    \"\"\"\n",
        "    Generates images using a DCGAN generator and saves each one individually.\n",
        "\n",
        "    Args:\n",
        "        generator (nn.Module): Trained DCGAN generator model\n",
        "        num_samples (int): Number of images to generate\n",
        "        save_dir (str): Directory to save generated images\n",
        "        prefix (str): Filename prefix for saved images\n",
        "    \"\"\"\n",
        "    generator.eval()\n",
        "    z = torch.randn(num_samples, z_dim, 1, 1).to(device)\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen_imgs = generator(z)\n",
        "        gen_imgs = gen_imgs * 0.5 + 0.5  # Denormalize from [-1, 1] to [0, 1]\n",
        "\n",
        "        for i, img in enumerate(gen_imgs):\n",
        "            save_path = os.path.join(save_dir, f\"{prefix}_{i:04d}.png\")\n",
        "            save_image(img, save_path)\n",
        "\n",
        "    print(f\"Saved {num_samples} images to '{save_dir}'\")\n",
        "    return gen_imgs"
      ],
      "metadata": {
        "id": "zeTqETdO8OYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_images_dcgan_individually(generator, num_samples=1024)"
      ],
      "metadata": {
        "id": "Xm9p2js08T5F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa6c341-cbbc-42bc-9391-bb396bbe8158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 1024 images to 'generated_images'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1.5657e-03, 1.3442e-03, 5.8168e-04,  ..., 5.6773e-05,\n",
              "           1.1784e-03, 3.1164e-03],\n",
              "          [1.2910e-03, 2.3162e-03, 4.2453e-04,  ..., 1.1325e-06,\n",
              "           4.9919e-05, 1.9804e-03],\n",
              "          [3.2893e-04, 8.6039e-05, 7.8291e-05,  ..., 8.9407e-08,\n",
              "           5.0545e-05, 1.3902e-03],\n",
              "          ...,\n",
              "          [1.3358e-03, 1.8689e-04, 1.0073e-05,  ..., 0.0000e+00,\n",
              "           1.4305e-06, 7.2151e-05],\n",
              "          [5.4428e-04, 6.7204e-05, 8.8215e-06,  ..., 3.2783e-07,\n",
              "           1.5378e-05, 5.8949e-05],\n",
              "          [7.9972e-04, 1.0911e-04, 1.6123e-05,  ..., 4.8071e-05,\n",
              "           1.1522e-04, 4.2784e-04]]],\n",
              "\n",
              "\n",
              "        [[[1.8896e-03, 6.4728e-04, 3.4165e-04,  ..., 3.0994e-06,\n",
              "           1.1185e-04, 9.5683e-04],\n",
              "          [7.4792e-04, 1.4174e-04, 6.0588e-05,  ..., 2.3842e-07,\n",
              "           7.0035e-06, 1.1083e-04],\n",
              "          [1.5318e-04, 1.5110e-05, 3.0220e-05,  ..., 0.0000e+00,\n",
              "           1.4603e-06, 3.0845e-05],\n",
              "          ...,\n",
              "          [1.4430e-04, 4.9174e-06, 2.4140e-06,  ..., 8.9407e-08,\n",
              "           1.1325e-06, 1.3113e-05],\n",
              "          [8.4043e-06, 2.0862e-07, 1.6391e-06,  ..., 9.8348e-07,\n",
              "           7.1824e-06, 7.1049e-05],\n",
              "          [9.5069e-06, 1.1623e-06, 1.3709e-06,  ..., 3.4541e-05,\n",
              "           2.7746e-05, 1.2726e-04]]],\n",
              "\n",
              "\n",
              "        [[[1.9333e-04, 1.9580e-05, 7.8857e-05,  ..., 1.5789e-04,\n",
              "           5.7834e-04, 1.6753e-03],\n",
              "          [4.7445e-05, 1.5497e-06, 3.3081e-06,  ..., 9.5963e-06,\n",
              "           9.0390e-05, 4.0141e-04],\n",
              "          [5.7667e-05, 1.1027e-06, 2.7716e-06,  ..., 5.6624e-07,\n",
              "           5.6267e-05, 2.3997e-04],\n",
              "          ...,\n",
              "          [8.2433e-05, 6.4373e-06, 2.9802e-07,  ..., 0.0000e+00,\n",
              "           1.0431e-06, 8.6427e-06],\n",
              "          [6.3032e-05, 1.6689e-06, 1.6689e-06,  ..., 8.9407e-08,\n",
              "           5.6624e-07, 3.3975e-06],\n",
              "          [1.7104e-04, 5.5641e-05, 5.4717e-05,  ..., 1.4216e-05,\n",
              "           1.1116e-05, 5.2571e-05]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[3.8030e-03, 2.4854e-03, 1.6875e-03,  ..., 2.3842e-06,\n",
              "           1.3292e-05, 1.3739e-04],\n",
              "          [1.7268e-03, 6.1619e-04, 4.3249e-04,  ..., 5.9605e-08,\n",
              "           5.3644e-07, 1.5199e-06],\n",
              "          [7.8988e-04, 5.5134e-05, 2.1040e-05,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 1.7881e-07],\n",
              "          ...,\n",
              "          [6.3813e-03, 5.2490e-02, 4.5811e-03,  ..., 3.7253e-06,\n",
              "           2.4655e-04, 1.3433e-03],\n",
              "          [1.2989e-02, 3.4364e-03, 1.0735e-03,  ..., 2.6822e-07,\n",
              "           1.1969e-04, 1.1660e-03],\n",
              "          [2.8078e-03, 7.9480e-04, 9.2164e-04,  ..., 1.0192e-05,\n",
              "           2.7671e-04, 1.5872e-03]]],\n",
              "\n",
              "\n",
              "        [[[1.0419e-03, 1.2425e-04, 4.6849e-05,  ..., 1.7151e-04,\n",
              "           3.6496e-04, 1.3683e-03],\n",
              "          [3.2506e-03, 3.1784e-04, 1.1566e-04,  ..., 2.9802e-07,\n",
              "           1.2964e-05, 2.8557e-04],\n",
              "          [5.6112e-04, 8.7589e-05, 7.5400e-05,  ..., 0.0000e+00,\n",
              "           3.1590e-06, 1.4371e-04],\n",
              "          ...,\n",
              "          [4.6191e-04, 2.8270e-04, 1.9908e-05,  ..., 3.8832e-05,\n",
              "           4.5723e-04, 4.3958e-04],\n",
              "          [2.1017e-04, 1.5914e-05, 1.0118e-04,  ..., 1.3411e-05,\n",
              "           6.6829e-04, 1.4447e-03],\n",
              "          [2.1747e-04, 5.7787e-05, 1.7571e-04,  ..., 3.9726e-05,\n",
              "           2.3761e-04, 5.2375e-04]]],\n",
              "\n",
              "\n",
              "        [[[1.1587e-03, 6.3759e-04, 3.0149e-03,  ..., 5.4227e-02,\n",
              "           5.6253e-02, 9.4603e-03],\n",
              "          [1.7861e-03, 8.2994e-04, 5.3436e-05,  ..., 7.7746e-03,\n",
              "           5.8498e-03, 3.3146e-03],\n",
              "          [6.4849e-03, 1.3408e-04, 2.5928e-06,  ..., 1.2076e-04,\n",
              "           5.5254e-04, 7.8174e-04],\n",
              "          ...,\n",
              "          [6.1500e-04, 8.9407e-06, 1.1325e-06,  ..., 0.0000e+00,\n",
              "           1.7881e-07, 5.8115e-06],\n",
              "          [5.2220e-04, 4.0025e-05, 7.2122e-06,  ..., 2.9802e-08,\n",
              "           2.9802e-07, 1.2517e-06],\n",
              "          [1.0871e-03, 8.7023e-05, 1.8686e-05,  ..., 6.2883e-06,\n",
              "           7.5102e-06, 6.1005e-05]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torchvision.utils import save_image\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def save_random_mnist_images(save_dir=\"real_images\", num_samples=1024, prefix=\"mnist\", batch_size=128):\n",
        "    \"\"\"\n",
        "    Extracts `num_samples` random MNIST images (from any class) and saves them individually.\n",
        "\n",
        "    Args:\n",
        "        save_dir (str): Folder to save the images\n",
        "        num_samples (int): Total number of images to save\n",
        "        prefix (str): Prefix for image filenames\n",
        "        batch_size (int): Batch size used for loading MNIST\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # MNIST normalization to [-1, 1] for FID\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    saved = 0\n",
        "    for batch_imgs, _ in loader:\n",
        "        for img in batch_imgs:\n",
        "            if saved >= num_samples:\n",
        "                print(f\"âœ… Saved {num_samples} MNIST images to '{save_dir}'\")\n",
        "                return\n",
        "            img_path = os.path.join(save_dir, f\"{prefix}_{saved:04d}.png\")\n",
        "            save_image(img, img_path)\n",
        "            saved += 1\n",
        "\n",
        "# Example usage:\n",
        "save_random_mnist_images()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajiE24Xvisky",
        "outputId": "3b17ba42-4982-4a19-d4e3-803add63c8ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 16.4MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 495kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 4.52MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 4.94MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved 1024 MNIST images to 'real_images'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FID Score Computation from Two Image Folders\n",
        "# ---------------------------------------------\n",
        "# This version computes FID by comparing real and generated images stored in folders\n",
        "# Requirements: pip install torchvision scipy tqdm\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class InceptionV3FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Load pretrained Inception with required weights\n",
        "        weights = Inception_V3_Weights.IMAGENET1K_V1\n",
        "        self.model = inception_v3(weights=weights, aux_logits=True)\n",
        "        self.model.eval()\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            if x.shape[2] != 299 or x.shape[3] != 299:\n",
        "                x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "            # Directly extract features before the final FC layer\n",
        "            x = self.model.Conv2d_1a_3x3(x)\n",
        "            x = self.model.Conv2d_2a_3x3(x)\n",
        "            x = self.model.Conv2d_2b_3x3(x)\n",
        "            x = self.model.maxpool1(x)\n",
        "            x = self.model.Conv2d_3b_1x1(x)\n",
        "            x = self.model.Conv2d_4a_3x3(x)\n",
        "            x = self.model.maxpool2(x)\n",
        "            x = self.model.Mixed_5b(x)\n",
        "            x = self.model.Mixed_5c(x)\n",
        "            x = self.model.Mixed_5d(x)\n",
        "            x = self.model.Mixed_6a(x)\n",
        "            x = self.model.Mixed_6b(x)\n",
        "            x = self.model.Mixed_6c(x)\n",
        "            x = self.model.Mixed_6d(x)\n",
        "            x = self.model.Mixed_6e(x)\n",
        "            x = self.model.Mixed_7a(x)\n",
        "            x = self.model.Mixed_7b(x)\n",
        "            x = self.model.Mixed_7c(x)\n",
        "            x = self.model.avgpool(x)  # Shape: [B, 2048, 1, 1]\n",
        "            x = x.view(x.size(0), -1)  # Shape: [B, 2048]\n",
        "            return x\n",
        "# ---------------------------------------------\n",
        "# Step 2: Load Images from Folder\n",
        "# ---------------------------------------------\n",
        "def get_image_loader(folder_path, batch_size=32):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299, 299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)  # Normalize to [-1, 1]\n",
        "    ])\n",
        "    dataset = datasets.ImageFolder(folder_path, transform=transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    return loader\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 3: Extract Features using InceptionV3\n",
        "# ---------------------------------------------\n",
        "def get_activations(dataloader, model, device):\n",
        "    model.eval()\n",
        "    activations = []\n",
        "    for images, _ in tqdm(dataloader, desc=\"Extracting features\"):\n",
        "        images = images.to(device)\n",
        "        feats = model(images)\n",
        "        activations.append(feats.cpu().numpy())\n",
        "    return np.concatenate(activations, axis=0)\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 4: Compute FrÃ©chet Distance (FID Formula)\n",
        "# ---------------------------------------------\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    diff = mu1 - mu2\n",
        "    covmean, _ = linalg.sqrtm(sigma1 @ sigma2, disp=False)  # Matrix square root of product of covariances\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "    return fid\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 5: Main Function to Compute FID\n",
        "# ---------------------------------------------\n",
        "def compute_fid_from_folders(real_folder, gen_folder, device):\n",
        "    # Load datasets\n",
        "    real_loader = get_image_loader(real_folder)\n",
        "    gen_loader = get_image_loader(gen_folder)\n",
        "\n",
        "    # Load feature extractor\n",
        "    feature_extractor = InceptionV3FeatureExtractor().to(device)\n",
        "\n",
        "    # Get activations for both sets\n",
        "    real_feats = get_activations(real_loader, feature_extractor, device)\n",
        "    gen_feats = get_activations(gen_loader, feature_extractor, device)\n",
        "\n",
        "    # Compute mean and covariance of activations\n",
        "    mu1, sigma1 = real_feats.mean(axis=0), np.cov(real_feats, rowvar=False)\n",
        "    mu2, sigma2 = gen_feats.mean(axis=0), np.cov(gen_feats, rowvar=False)\n",
        "\n",
        "    # Compute the FID score\n",
        "    fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
        "    return fid\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Example usage (replace with your paths)\n",
        "# ---------------------------------------------\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    real_path = \"real_images\"      # Folder containing real images (must be in subfolder, e.g., real_images/class_x/)\n",
        "    generated_path = \"generated_images\"  # Folder containing generated images (same structure)\n",
        "\n",
        "    fid_score = compute_fid_from_folders(real_path, generated_path, device)\n",
        "    print(f\"FID Score: {fid_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL9z9q3Zi4ii",
        "outputId": "8c0e3ad2-1b02-4561-c36c-1c310e64fc00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [06:41<00:00, 12.55s/it]\n",
            "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [06:45<00:00, 12.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID Score: 39.2303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_dir = os.getcwd()\n",
        "folders = [f for f in os.listdir(current_dir) if os.path.isdir(os.path.join(current_dir, f))]\n",
        "\n",
        "print(\"ðŸ“ Folders in current directory:\")\n",
        "for folder in folders:\n",
        "    print(f\" - {folder}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2164ZVtj2Bs",
        "outputId": "495f5f7f-d1b1-4774-f779-e0bb76bea412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“ Folders in current directory:\n",
            " - .config\n",
            " - real_images\n",
            " - data\n",
            " - MNIST\n",
            " - generated_images\n",
            " - sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def wrap_in_class_folder(base_dir, class_name=\"class_x\"):\n",
        "    class_dir = os.path.join(base_dir, class_name)\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "    for fname in os.listdir(base_dir):\n",
        "        path = os.path.join(base_dir, fname)\n",
        "        if os.path.isfile(path):\n",
        "            shutil.move(path, os.path.join(class_dir, fname))\n",
        "\n",
        "# Example usage\n",
        "wrap_in_class_folder(\"real_images\")\n",
        "wrap_in_class_folder(\"generated_images\")"
      ],
      "metadata": {
        "id": "h1IPI5rakRlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "for folder in [\"real_images\", \"generated_images\"]:\n",
        "    checkpoints = os.path.join(folder, \".ipynb_checkpoints\")\n",
        "    if os.path.exists(checkpoints):\n",
        "        shutil.rmtree(checkpoints)\n",
        "        print(f\"Removed: {checkpoints}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQlpIKeMkh_P",
        "outputId": "e1654c3a-81a0-4a1d-a84d-677a3464450e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed: generated_images/.ipynb_checkpoints\n"
          ]
        }
      ]
    }
  ]
}